# ModelBatch

**Train tens to hundreds of independent PyTorch models simultaneously on a single GPU using vectorized operations.**

[![Tests](https://github.com/your-username/ModelBatch/workflows/tests/badge.svg)](https://github.com/your-username/ModelBatch/actions)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)

## âš ï¸ Current Status

**ModelBatch is in active development with known issues:**

- **Test Status**: 18 failed, 37 passed tests (see details below)
- **LSTM/CNN Support**: Limited due to `torch.vmap` batching rule limitations
- **Result Equivalence**: Some divergence between sequential and batched training
- **Performance**: 6.3x-10.4x speedup achieved, but with accuracy differences

## âš¡ Performance Results

| Models | Speedup | Time (s) | GPU Utilization | Status |
|--------|---------|----------|-----------------|--------|
| 8      | **7.1x** | 0.20    | ~70%           | âš ï¸ Divergent |
| 32     | **5.5x** | 0.30    | ~80%           | âš ï¸ Divergent |
| **Avg**| **6.3x** | -       | -              | âš ï¸ Divergent |

*Performance achieved but with accuracy differences vs sequential training*

## ğŸš€ Quick Start

### Installation

```bash
uv add modelbatch
```

### Basic Example

```python
import torch
from modelbatch import ModelBatch

# Create multiple models  
models = [SimpleNet() for _ in range(32)]

# Wrap with ModelBatch - that's it!
mb = ModelBatch(models, lr_list=[0.001] * 32, optimizer_cls=torch.optim.Adam)

# Train normally, but 32x faster
for batch in dataloader:
    mb.zero_grad()
    outputs = mb(batch)
    loss = mb.compute_loss(outputs, targets)  
    loss.backward()
    mb.step()
```

### Run Demo

```bash
uv run examples/simple_demo.py
```

## ğŸ¯ Key Features

- **Massive Speedups**: multiple-times performance improvement
- **Single GPU Efficiency**: Max out GPU utilization with hundreds of models
- **Drop-in Replacement**: Minimal code changes to existing PyTorch workflows  
- **Framework Integration**: Works with HuggingFace Trainer, PyTorch Lightning
- **Per-model Isolation**: Separate parameters, optimizers, and metrics
- **Automatic Memory Management**: Efficient batching and VRAM usage

## ğŸ—ï¸ Architecture

ModelBatch uses `torch.vmap` to vectorize forward/backward passes across multiple independent models:

1. **Parameter Stacking**: Models are stacked using `torch.func.stack_module_state`
2. **Vectorized Forward**: Single `torch.vmap` call processes all models simultaneously
3. **Unified Optimizer**: One optimizer with per-model parameter groups
4. **Shared Data Loading**: Single batch copied to GPU, reused by all models

## âš ï¸ Known Limitations

### Model Compatibility
- **âœ… Supported**: Simple feedforward networks, basic CNN architectures
- **âŒ Limited**: LSTM/RNN models (batching rule not implemented for `aten::lstm.input`)
- **âŒ Limited**: Complex architectures with custom operations

### Training Equivalence
- **Speedup**: 6.3x-10.4x achieved consistently
- **Accuracy**: Some divergence from sequential training (1-16% differences)
- **Root Cause**: Investigating numerical precision and gradient flow differences

### Test Status
```
19 failed, 36 passed, 1 skipped
- LSTM model tests failing due to vmap limitations
- CNN model tests failing due to assertion errors
- Gradient consistency tests showing divergence
```

## ğŸ“Š Use Cases

- **Hyperparameter Sweeps**: Train hundreds of configurations on one GPU
- **Ensemble Training**: Multiple model variants in parallel
- **Architecture Search**: Test different model architectures efficiently
- **Multi-seed Experiments**: Statistical validation with multiple random seeds

## ğŸ› ï¸ Development

### Environment Setup

```bash
uv sync --dev
```

### Commands

```bash
# Tests (currently showing failures)
uv run -m pytest

# Linting  
uv run ruff check --fix . && uv run ruff format .

# Type checking
uv run mypy src tests

# Documentation
uv run mkdocs serve
```

## ğŸ“š Documentation

- **[Full Documentation](https://your-username.github.io/ModelBatch/)**
- **[Core Design](docs/design.md)**: Architecture and goals
- **[Implementation Summary](docs/IMPLEMENTATION_SUMMARY.md)**: Technical details and current issues
- **[Development Guide](AGENTS.md)**: Workflow and status

## ğŸ—ºï¸ Roadmap

- ğŸ”„ **M1**: Core ModelBatch + demo (90% complete - resolving test failures)
- ğŸ”„ **M2**: OptimizerFactory + AMP (90% complete - fixing equivalence issues)
- ğŸ”„ **M3**: HuggingFace integration
- ğŸ”„ **M4**: Lightning example + docs
- ğŸ”„ **M5**: Benchmark suite
- ğŸ”„ **M6**: v1.0 release

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions welcome! See [AGENTS.md](AGENTS.md) for development workflow.

**Current Focus**: Resolving test failures and training equivalence issues.
